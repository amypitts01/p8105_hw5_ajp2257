---
title: "p8105_hw5_ajp2257"
author: "Amy Pitts"
date: "11/8/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)
```


# Problem 1

```{r, message=FALSE, warning= FALSE}
homicide_df = read_csv("homicide-data.csv") %>%
  mutate( 
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when( # new variable that depends on disposition 
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>%
  select(city_state, resolved) %>%
  filter(city_state != "Tulsa_AL")
```


```{r}
agg_df = homicide_df %>%
  group_by(city_state) %>%
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  ) 
```


```{r}
prop.test(
  agg_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved), 
  agg_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy() 
```

try to iterate 
```{r}
results = agg_df %>%
    mutate(
      prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)), # map over both vars 
      tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
    ) %>%
  select(-prop_tests) %>%
  unnest(tidy_tests) %>%
  select(city_state , estimate, conf.low, conf.high)
results 
```
Plotting the results 
```{r}
results %>%
  mutate(
    city_state  = fct_reorder(city_state, estimate)
  ) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1) )
```



# Problem 2
This zip file contains data from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm.

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time:

* Start with a dataframe containing all file names; the list.files function will help
* Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe
* Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary
* Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

```{r message = FALSE}
files = list.files("data", pattern = "*.csv") # get the file name from the data directory 
data  = files %>%                             # reading the data from each file 
  map(~ read_csv(file.path("data", .)))  %>%  # read in all the files by iterating 
  reduce(rbind)                               # getting the results from the files 

# getting the arm and the id from the file names 
participant = separate(data.frame(files), col=files, sep = "([._])", into =c("arm", "id")) 

# to be able to plot the data we need a values columns and a time column 
participants_df = tibble(participant, data ) %>% # combining the data and the arm and id 
  pivot_longer(                                  # we are pivoting all the variables 
    cols = week_1:week_8,
    names_prefix = "week_",
    names_to = "week",
    values_to = "value"
  )
head(participants_df) # quick view of the data 
```

Plotting time! 

```{r}
# looking at spagetti plot over time (in weeks) by the two different arms 
participants_df %>%
  mutate(
    uniq = str_c(arm , id, sep="_") #making a unique identifier for each person 
  ) %>%
ggplot( aes(x = week, y = value, group = uniq)) + 
  geom_line(aes(color=arm)) + # coloring the participants by arms
  labs(
    title = "A Spaghetti plot showing each participant over time (weeks) by group."
  )
```

In looks like in the control group the values seem to stay on the lower side never going above about 4. The experiment group starts with values on the lower side just like the control group but then the values increase over time. In the later weeks it looks like the control group and the experiment group are different with the experiment group having bigger values. 


# Problem 3
When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a simple linear regression.

First set the following design elements:

* Fix n=30
* Fix sigma=5

Set mu=0. Generate 5000 datasets from the model x ~ Normal[mu, sigma]. For each dataset, save mu hat  and the p-value arising from a test of H:mu =0 using alpha=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.

Repeat the above for mu={1,2,3,4,5,6}, and complete the following:

* Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu on the x axis. Describe the association between effect size and power.
* Make a plot showing the average estimate of hat{mu}  on the y axis and the true value of μ on the x axis. Make a second plot (or overlay on the first) the average estimate of hat{mu}  \textbf{only in samples for which the null was rejected} on the y axis and the true value of mu on the x axis. Is the sample average of hat{mu} across tests for which the null is rejected approximately equal to the true value of mu? Why or why not?

The is setting up the data generation 
```{r}
set.seed(1) # for reproducability 
# setting up variables given in the directions
n     = 30   # size of the random sample 
sigma = 5    # the variance 
alpha = 0.05 # the error threshold 

# generating the data and running a t-test to get the simulation estimates 
sim_est = function(mu) {                 # input is just the true mu 
  sim_data = tibble(                     # storing the data in the tibble 
    x = rnorm(n, mean = mu, sd = sigma), # this is the simulated data from a normal distribution 
  )
  # doing the t.test and getting the results
  t.test(sim_data, mu = 0) %>% # comparing sample against mu = 0 
    broom::tidy() %>%          # making the outputs into a tibble 
    select(estimate, p.value)  # getting the estimate and the p-value 
}

```

Iterating through all the mu values and running the data generation 5000 times 
```{r}
# running the simulation 5000 times and for each mu value 
sim_results = 
  tibble(mu = c(0:6)) %>%  # these are all the mus we want to try 
  mutate(                  # running the simulation we made above with different mu values 
    output_lists = map(.x = mu, ~rerun(5000, sim_est(mu = .x))), # running data gen 5000 times
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu on the x axis. Describe the association between effect size and power.

```{r}
# plot showing the proportion of times the null was rejected (the power of the test)
sim_results %>% 
   mutate( # updating the mu variable to have it plot in a nice way 
    mu = str_c("mu = ", mu),
    mu = fct_inorder(mu),
    reject_status = ifelse(p.value > alpha, 0, 1) # reject null = 1 based on alpha 
    ) %>% 
  group_by(mu) %>% # group by mu to count how many times we reject based on alpha 
  count(reject_status) %>%
  filter(reject_status == 1) %>%     # filter just the reject null (when = 1)
   ggplot(aes(x = mu, y = n/5000)) + # I am dividing by 5000 to get the proportion 
    geom_col(aes(fill=mu)) +
    #geom_hline(yintercept=0.05, linetype="dashed", size=1) +
    geom_text(aes(label=round(n/5000, 3)), position = position_nudge(y = 0.02)) + 
    labs(
      title = "The Proportion of times null was rejected with different Means",
      y = "Proportion of times null was rejected"
    )
```

We see here that when mu=0 we get the lowest amount of times the null was rejected. That makes sense because we are running a hypothesis test seeing if the sample is close to having a population mu of 0. This is our alpha value or a false positive rate. When we increase the mu value then the number of times the null was rejected increases. When mu equal 5 and 6 then we are almost always rejecting the null hypothesis. 

Make a plot showing the average estimate of hat{mu}  on the y axis and the true value of μ on the x axis. Make a second plot (or overlay on the first) the average estimate of hat{mu}  \textbf{only in samples for which the null was rejected} on the y axis and the true value of mu on the x axis.
```{r}
# average estimates against the true mu value graph 
mean_values = sim_results %>% 
   mutate( # Changing the mu variable to make it better to graph 
    mu = str_c("mu = ", mu), 
    mu = fct_inorder(mu),
    ) %>% 
  group_by(mu) %>% # want to group the means by the mu values 
  summarize(mean_est = mean(estimate)) %>% # getting the mean 
    # now we are plotting 
    ggplot(aes(x = mu, y = mean_est)) + 
    geom_point(aes(color=mu), size=4) +
    geom_text(aes(label=round(mean_est, 3)), position = position_nudge(y = 0.4)) + 
    labs(
      title = "Estimates vs Truth",
      y = "Mean of the Estimates",
      x = "True Mu"
    ) + 
   theme(legend.position = "none")

# just looking at the estimates for the times were rejected the null against the true mu
resticted_means = sim_results %>% 
   mutate( # Changing the mu variable to make it better to graph 
    mu = str_c("mu = ", mu),
    mu = fct_inorder(mu),
    reject_status = ifelse(p.value > alpha, 0, 1) # reject null = 1 based on alpha 
    ) %>% 
  group_by(mu, reject_status) %>% # grouping by mu and the reject_status 
  summarize(mean_est = mean(estimate)) %>% # sumarizing the results 
  filter(reject_status == 1) %>%
    # plotting the results 
    ggplot(aes(x = mu, y = mean_est)) +
    geom_point(aes(color=mu), size=4) +
    geom_text(aes(label=round(mean_est, 3)), position = position_nudge(y = 0.4)) + 
    labs(
      title = "Estimates vs Truth of only the rejections",
      y = "Mean of the Estimates",
      x = "True Mu"
    ) +
   theme(legend.position = "none")

# plotting them together 
mean_values + resticted_means
```

Here we are looking at two graphs the one on the left is the mean estimate over all the 5000 samples split up by each mu values. The second graph on the right is just looking a the mean estimate for the times we rejected the null hypothesis. First, on the left, we see that the mean estimates are very close to the true mu value. Looking at the graph with only the rejection mean estimates we see that when mu equal 0 we have an estimate of almost 0 (0.43). However, there is a huge jump for when the true mu is 1, the estimate is 2.214 which is very distant from mu=0 so that makes sense why we are rejecting out null hypothesis. Looking at the estimates of mu = 4, 5, and 6 we see that the estimates are very close to the true mu. This makes sense because that value is very far from 0 so we are almost always rejecting the null hypothesis. 














